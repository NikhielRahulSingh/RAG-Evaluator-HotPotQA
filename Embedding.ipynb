{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load HotPotQA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>actual_contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hard</td>\n",
       "      <td>George Boscawen, 9th Viscount Falmouth is a fo...</td>\n",
       "      <td>the Guards Division, Foot Guards regiments</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hard</td>\n",
       "      <td>When Vladimir Kashpur portrayed Baba Yaga she ...</td>\n",
       "      <td>trio of sisters</td>\n",
       "      <td>[10, 11, 12, 13, 14, 15, 16, 17, 18, 19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hard</td>\n",
       "      <td>Which musician has a solo punk rock project: T...</td>\n",
       "      <td>Frank Anthony Iero, Jr.</td>\n",
       "      <td>[20, 21, 22, 23, 24, 25, 26, 27, 28, 29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hard</td>\n",
       "      <td>A Disney voice actor has won which Emmy award?</td>\n",
       "      <td>Outstanding Supporting Actor</td>\n",
       "      <td>[30, 31, 32, 33, 34, 35, 36, 37, 38, 39]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hard</td>\n",
       "      <td>Which north-western suburb of Adelaide lies wi...</td>\n",
       "      <td>Birkenhead</td>\n",
       "      <td>[40, 41, 42, 43, 44, 45, 46, 47, 48, 49]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  level                                           question  \\\n",
       "0  hard  George Boscawen, 9th Viscount Falmouth is a fo...   \n",
       "1  hard  When Vladimir Kashpur portrayed Baba Yaga she ...   \n",
       "2  hard  Which musician has a solo punk rock project: T...   \n",
       "3  hard     A Disney voice actor has won which Emmy award?   \n",
       "4  hard  Which north-western suburb of Adelaide lies wi...   \n",
       "\n",
       "                                       answer  \\\n",
       "0  the Guards Division, Foot Guards regiments   \n",
       "1                             trio of sisters   \n",
       "2                     Frank Anthony Iero, Jr.   \n",
       "3                Outstanding Supporting Actor   \n",
       "4                                  Birkenhead   \n",
       "\n",
       "                            actual_contexts  \n",
       "0            [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  \n",
       "1  [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]  \n",
       "2  [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]  \n",
       "3  [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]  \n",
       "4  [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils.hotpot_data_loader import HotPotQA\n",
    "\n",
    "SEED = 42\n",
    "SAMPLE = 500\n",
    "DIFFICULTY = \"500\"\n",
    "\n",
    "contexts, hotpot_qa_df = HotPotQA(SAMPLE=SAMPLE, DIFFICULTY=DIFFICULTY, SEED=SEED).get_data()\n",
    "hotpot_qa_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check machine configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu124\n",
      "CUDA Version:  12.4\n",
      "Device name: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "FlashAttention available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "print(\"CUDA Version: \", torch.version.cuda)\n",
    "print(\"Device name:\", torch.cuda.get_device_properties(\"cuda\").name)\n",
    "print(\"FlashAttention available:\", torch.backends.cuda.flash_sdp_enabled())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Embeddding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from utils.embedders.sentence_transformer_embedder import SentenceTransformerEmbedder\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.schema import QueryBundle\n",
    "\n",
    "embedder = SentenceTransformerEmbedder(huggingface_token = \"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\",\n",
    "                                       cache_loc         = \"D:/Users/nikhi/.cache/huggingface/hub/\",\n",
    "                                       model_save_loc    = \"D:/Users/nikhi/hugging_face_embedding_models\")\n",
    "\n",
    "model_name=\"dunzhang/stella_en_400M_v5\" #dunzhang/stella_en_1.5B_v5\n",
    "# embedder.download_embedding_model(model_name=model_name)\n",
    "\n",
    "model = SentenceTransformer(model_name_or_path = f\"D:/Users/nikhi/hugging_face_embedding_models/{model_name}\", \n",
    "                            trust_remote_code=True,\n",
    "                            device=\"cuda\",\n",
    "                            model_kwargs={\"attn_implementation\": \"eager\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f60de3872724073940e82a09cc935a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_prompt_name = \"s2p_query\"\n",
    "queries = [(doc.text if isinstance(doc, TextNode) else doc.query_str) for doc in hotpot_qa_df['question']]\n",
    "query_embeddings = model.encode(queries,show_progress_bar=True,prompt_name=query_prompt_name)\n",
    "for embedding,query in zip(query_embeddings,hotpot_qa_df['question']):\n",
    "    query.embedding = embedding.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed Actual Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3590bdb11e4486a2adff7b00d5b565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/156 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "docs = [(doc.text if isinstance(doc, TextNode) else doc.query_str) for doc in contexts.values()]\n",
    "doc_embeddings = model.encode(docs,show_progress_bar=True)\n",
    "for embedding,document in zip(doc_embeddings,contexts.values()):document.embedding = embedding.tolist()\n",
    "context_similarities = model.similarity(doc_embeddings,doc_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "save_dir = f\"embeddings/{model_name}/{DIFFICULTY}/{SAMPLE}\"\n",
    "\n",
    "embedder.save_embeddings(contexts=contexts,\n",
    "                         df=hotpot_qa_df,\n",
    "                         df_name = f'df',\n",
    "                         similarity_matrix=context_similarities,\n",
    "                         save_dir=save_dir)\n",
    "\n",
    "hotpot_qa_df = pd.read_pickle(f'{save_dir}/df.pkl')\n",
    "with open(f'{save_dir}/contexts.pkl', 'rb') as f:contexts = pickle.load(f)\n",
    "with open(f'{save_dir}/similarity_matrix.pkl', 'rb') as f:context_similarities = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "\n",
    "# Create a linkage matrix\n",
    "Z = hierarchy.linkage(dissimilarity_matrix, method='single')\n",
    "print(\"done\")\n",
    "# Create a dendrogram\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "dn = hierarchy.dendrogram(Z, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icoord = dn['icoord']\n",
    "dcoord = dn['dcoord']\n",
    "leaves = dn['leaves']\n",
    "\n",
    "default_threshold = 0.7 * np.max(Z[:, 2])\n",
    "# Identify clusters based on merge points\n",
    "blue_connector_nodes = []\n",
    "\n",
    "# Iterate over dcoord to find the high merges (blue connectors)\n",
    "for i, ys in enumerate(dcoord):\n",
    "    if ys[1] == ys[2] and ys[1] > default_threshold:  # Use default threshold\n",
    "        merge_height = ys[1]\n",
    "        # Get the x-coordinates (clusters) being merged\n",
    "        x_cluster = (icoord[i][1], icoord[i][2])\n",
    "        \n",
    "        # Map x-coordinates to leaf node indices\n",
    "        # Each `icoord` x-value is an x-axis position for the corresponding leaf node\n",
    "        # We'll map these x-values back to the leaf indices\n",
    "        node1 = int(np.round((x_cluster[0] - 5) / 10.0))  # Mapping back to leaf\n",
    "        node2 = int(np.round((x_cluster[1] - 5) / 10.0))  # Same for second cluster\n",
    "        \n",
    "        # Use the mapped values to get the original indices from leaves\n",
    "        leaf1 = leaves[node1]\n",
    "        leaf2 = leaves[node2]\n",
    "        \n",
    "        blue_connector_nodes.append(((leaf1, leaf2), merge_height))\n",
    "\n",
    "print(\"Nodes linked by blue connectors:\")\n",
    "for _ in blue_connector_nodes:\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node_id,cluster in zip(dn['leaves'],dn['leaves_color_list']):\n",
    "    print(f\"node:{node_id} is in cluster {cluster}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create two graphs\n",
    "G1 = nx.Graph()\n",
    "G2 = nx.Graph()\n",
    "\n",
    "# Add nodes and edges to both graphs\n",
    "G1.add_nodes_from([1, 2, 3])\n",
    "G1.add_edge(1, 2)\n",
    "\n",
    "G2.add_nodes_from([4, 5, 6])\n",
    "G2.add_edge(4, 5)\n",
    "\n",
    "# Merge the two graphs\n",
    "G = nx.compose(G1, G2)\n",
    "\n",
    "# Add an edge between a node from G1 and a node from G2\n",
    "G.add_edge(2, 4)\n",
    "\n",
    "# Plot the graphs\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Draw G1\n",
    "nx.draw(G1, with_labels=True, ax=axes[0], node_color='lightblue', edge_color='gray')\n",
    "axes[0].set_title(\"Graph G1\")\n",
    "\n",
    "# Draw G2\n",
    "nx.draw(G2, with_labels=True, ax=axes[1], node_color='lightgreen', edge_color='gray')\n",
    "axes[1].set_title(\"Graph G2\")\n",
    "\n",
    "# Draw merged graph G\n",
    "nx.draw(G, with_labels=True, ax=axes[2], node_color='lightcoral', edge_color='gray')\n",
    "axes[2].set_title(\"Merged Graph G\")\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Create a graph from the linkage matrix\n",
    "G = nx.Graph()\n",
    "for i in range(len(Z)):\n",
    "    # Get the indices of the two clusters that were merged at this step\n",
    "    cluster1, cluster2 = Z[i, :2].astype(int)\n",
    "\n",
    "    # Add the clusters as nodes to the graph\n",
    "    G.add_node(cluster1)\n",
    "    G.add_node(cluster2)\n",
    "\n",
    "    # Add an edge between the clusters\n",
    "    G.add_edge(cluster1, cluster2)\n",
    "\n",
    "# Draw the graph\n",
    "visualize_graph(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_graph(G, filename=\"network_graph.html\"):\n",
    "    # Generate positions for nodes using the spring layout\n",
    "    pos = nx.spring_layout(G,k=0.25)\n",
    "\n",
    "    # Create edge traces\n",
    "    edge_trace = []\n",
    "    for edge in G.edges(data=True):\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        weight = round(edge[2].get('weight', 1), 3)\n",
    "        edge_trace.append(\n",
    "            go.Scatter(\n",
    "                x=[x0, x1, None], y=[y0, y1, None],\n",
    "                line=dict(width=0.5, color='#888'),\n",
    "                hoverinfo='text',\n",
    "                text=f'Weight: {weight}',\n",
    "                mode='lines'\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Initialize node trace with empty lists for x, y, and text\n",
    "    node_trace = go.Scatter(\n",
    "        x=[], y=[],\n",
    "        mode='markers',\n",
    "        hoverinfo='text',\n",
    "        text=[],  # Initialize as an empty list\n",
    "        marker=dict(\n",
    "            showscale=True,\n",
    "            colorscale='YlGnBu',\n",
    "            color=[],\n",
    "            size=10,\n",
    "            colorbar=dict(\n",
    "                thickness=15,\n",
    "                title='Node Connections',\n",
    "                xanchor='left',\n",
    "                titleside='right'\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    node_adjacencies = []\n",
    "    node_texts = []  # Initialize a separate list for node texts\n",
    "    for node in G.nodes():\n",
    "        x, y = pos[node]\n",
    "        node_trace['x'] += tuple([x])\n",
    "        node_trace['y'] += tuple([y])\n",
    "\n",
    "        # Number of connections (degree)\n",
    "        node_adjacencies.append(len(G.adj[node]))\n",
    "        adj_nodes = G.adj[node]\n",
    "        node_texts.append(f'{node}:# of connections: {len(adj_nodes)} \\n{list(dict(adj_nodes).keys())}')  # Add to node_texts list\n",
    "\n",
    "    node_trace.marker.color = node_adjacencies\n",
    "    node_trace.text = node_texts  # Set the text attribute\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure(data=edge_trace + [node_trace],\n",
    "                    layout=go.Layout(\n",
    "                        title='<br>Network graph visualization',\n",
    "                        titlefont_size=16,\n",
    "                        showlegend=False,\n",
    "                        hovermode='closest',\n",
    "                        margin=dict(b=0, l=0, r=0, t=40),\n",
    "                        annotations=[dict(\n",
    "                            text=\"Graph Visualization with Plotly\",\n",
    "                            showarrow=False,\n",
    "                            xref=\"paper\", yref=\"paper\",\n",
    "                            x=0.005, y=-0.002)],\n",
    "                        xaxis=dict(showgrid=False, zeroline=False),\n",
    "                        yaxis=dict(showgrid=False, zeroline=False))\n",
    "                    )\n",
    "\n",
    "    # Save the figure as an HTML file\n",
    "    pio.write_html(fig, file=filename, auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np \n",
    "\n",
    "model_DB = DBSCAN(metric = 'cosine').fit(context_similarities.cpu())\n",
    "labels = model_DB.labels_\n",
    "\n",
    "unique, counts = np.unique(labels, return_counts = True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "def add_node_to_graph(G, node):\n",
    "    G.add_node(int(node.id_), node=node)\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(add_node_to_graph, G, chunk) for chunk in list(contexts.values())]\n",
    "    for future in tqdm(futures, total=len(futures),desc=f\"adding contexts...\"):\n",
    "        future.result()\n",
    "\n",
    "chunk_similarities_cpu = context_similarities.cpu()\n",
    "indices = torch.triu_indices(chunk_similarities_cpu.shape[0], chunk_similarities_cpu.shape[1], offset=1, device='cpu')\n",
    "filtered_weights = chunk_similarities_cpu[indices[0], indices[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_indices_list = [(int(i), int(j), float(w)) for i, j, w in zip(indices[0], indices[1], filtered_weights)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in filtered_indices_list:\n",
    "    print(_)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.embedding_stores.graph import graph_db\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# THRESHOLD = 0.0\n",
    "contexts_list = list(contexts.values())\n",
    "\n",
    "# G1 = graph_db.create_graph(contexts_list,context_similarities,THRESHOLD,save_dir+f\"/graph_store/{THRESHOLD}\",True)\n",
    "# G1 = graph_db.load_graph(save_dir+f\"/graph_store/{THRESHOLD}\")\n",
    "\n",
    "\n",
    "# G2 = nx.Graph()\n",
    "\n",
    "# nodes = contexts_list\n",
    "# for node in nodes:\n",
    "#     G2.add_node(node.id_, node=node)\n",
    "\n",
    "# # Add edges to the graph based on cosine similarity\n",
    "# total_iterations = sum(range(len(contexts)))\n",
    "\n",
    "# for i in tqdm(range(len(contexts_list)), desc=\"Outer Loop\"):\n",
    "#     for j in range(i+1, len(contexts_list)):\n",
    "#         if context_similarities[i, j] > THRESHOLD:\n",
    "#             G2.add_edge(contexts_list[i].id_,contexts_list[j].id_,weight=context_similarities[i, j])\n",
    "\n",
    "\n",
    "# if G1.number_of_nodes() != G2.number_of_nodes():\n",
    "#     print(\"The graphs are not the same: different number of nodes\")\n",
    "# elif G1.number_of_edges() != G2.number_of_edges():\n",
    "#     print(\"The graphs are not the same: different number of edges\")\n",
    "# else:\n",
    "#     # Check if the nodes in the graphs are the same\n",
    "#     if set(G1.nodes()) != set(G2.nodes()):\n",
    "#         print(\"The graphs are not the same: different nodes\")\n",
    "#     else:\n",
    "#         # Check if the edges in the graphs are the same\n",
    "#         if set(G1.edges()) != set(G2.edges()):\n",
    "#             print(\"The graphs are not the same: different edges\")\n",
    "#         else:\n",
    "#             # Check if the attributes of the nodes and edges are the same\n",
    "#             for node in G1.nodes():\n",
    "#                 if G1.nodes[node] != G2.nodes[node]:\n",
    "#                     print(\"The graphs are not the same: different attributes for node\", node)\n",
    "#                     break\n",
    "#             else:\n",
    "#                 for edge in G1.edges():\n",
    "#                     if G1.edges[edge] != G2.edges[edge]:\n",
    "#                         print(\"The graphs are not the same: different attributes for edge\", edge)\n",
    "#                         break\n",
    "#                 else:\n",
    "#                     print(\"The graphs are the same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from utils.embedding_stores.graph import graph_db\n",
    "\n",
    "from typing import List\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.schema import QueryBundle\n",
    "\n",
    "def create_retriever(chunk_nodes,k):\n",
    "\n",
    "\n",
    "    bm25_retriever = BM25Retriever.from_defaults(nodes=chunk_nodes, similarity_top_k=k)\n",
    "\n",
    "    print(f\"BM-25 Retriever created\")\n",
    "    return bm25_retriever    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.embedding_retrievers import bm25\n",
    "from utils.embedding_retrievers.graph import a_star,bfs\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "\n",
    "def retrieve_chunks_bm25(db,query_bundles:QueryBundle):\n",
    "    \n",
    "    chunks = bm25.perform_retrieval(query_bundles,db)\n",
    "    \n",
    "    with open('.tmp/benchmarks.pkl', 'rb') as f: \n",
    "        benchmarks_dict = pickle.load(f)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        ids = [int(t_node.node.id_) for t_node in chunk]\n",
    "        benchmarks_dict[\"retrieved_contexts\"].append(ids)\n",
    "\n",
    "    df_benchmark = pd.DataFrame(benchmarks_dict)\n",
    "    return df_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.embedding_retrievers.vector import vec_similarity\n",
    "from utils.embedding_stores.vector import vec_db\n",
    "\n",
    "V = vec_db.create_or_load_vector_db(db_name=f\"{DIFFICULTY}_{SAMPLE}\",\n",
    "                                        save_loc=save_dir+f\"/vector_store\",\n",
    "                                        docs=contexts_list)\n",
    "\n",
    "hotpot_qa_df['vec_retrieved'] = vec_similarity.perform_retrieval(hotpot_qa_df,V)\n",
    "hotpot_qa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.embedding_stores.graph import graph_db\n",
    "from utils.embedding_retrievers.graph import a_star\n",
    "\n",
    "THRESHOLD = 0.5\n",
    "G = graph_db.create_graph(contexts_list,context_similarities,THRESHOLD,save_dir+f\"/graph_store/{THRESHOLD}\",True)\n",
    "G = graph_db.load_graph(save_dir+f\"/graph_store/{THRESHOLD}\")\n",
    "nodes_with_score = a_star.perform_retrieval(hotpot_qa_df,G,THRESHOLD)\n",
    "node_ids = [[str(node.id_) for node in node_with_score[:10]] for node_with_score in nodes_with_score]\n",
    "hotpot_qa_df['a_star_retrieved'] = node_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation_metrics.retriever import RetrieverEvaluator\n",
    "\n",
    "evaluator = RetrieverEvaluator(hotpot_qa_df,'a_star_retrieved')\n",
    "\n",
    "order_unaware_metrics = evaluator.get_order_unaware_metrics(k=10) \n",
    "order_aware_metrics = evaluator.get_order_aware_metrics() \n",
    "\n",
    "print(f\"\\nA star:\")\n",
    "print(f\"order unaware metrics : {order_unaware_metrics}\")\n",
    "print(f\"order aware metrics   : {order_aware_metrics}\")\n",
    "\n",
    "evaluator = RetrieverEvaluator(hotpot_qa_df,'vec_retrieved')\n",
    "\n",
    "order_unaware_metrics = evaluator.get_order_unaware_metrics(k=10) \n",
    "order_aware_metrics = evaluator.get_order_aware_metrics()\n",
    "\n",
    "print(f\"\\nVec sim:\")\n",
    "print(f\"order unaware metrics : {order_unaware_metrics}\")\n",
    "print(f\"order aware metrics   : {order_aware_metrics}\")\n",
    "hotpot_qa_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def calculate_cosine_similarity_variance(nodes):\n",
    "    # Ensure that the list of nodes contains at least two elements\n",
    "    if len(nodes) < 2:\n",
    "        return -1\n",
    "        \n",
    "    embeddings = [contexts[str(node)].embedding for node in nodes]\n",
    "    similarities = cosine_similarity(embeddings)\n",
    "    similarities = similarities.flatten()\n",
    "    mean = np.mean(similarities)\n",
    "    variance = np.var(similarities, ddof=1)\n",
    "\n",
    "    return variance\n",
    "\n",
    "def get_clusters(community):\n",
    "    res = {}\n",
    "    x = community.to_node_community_map()\n",
    "\n",
    "    res = {}\n",
    "    for key,value in zip(x.keys(),x.values()):\n",
    "        value = value[0]\n",
    "        if value not in res:\n",
    "            res[value] = []\n",
    "        res[value].append(int(key))\n",
    "    \n",
    "    return res\n",
    "\n",
    "def get_cluster_variance(clusters):\n",
    "    results = {}\n",
    "    for cluster in list(clusters.keys()):\n",
    "        nodes = clusters[cluster]\n",
    "        variance = calculate_cosine_similarity_variance(nodes)\n",
    "        results[cluster] = variance\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_clusters(community):\n",
    "\n",
    "    clusters = get_clusters(community=community)\n",
    "    results = get_cluster_variance(clusters)\n",
    "\n",
    "    x = list(results.keys())\n",
    "    y = list(results.values())\n",
    "\n",
    "    bar_data = []\n",
    "    scatter_data = []\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        if y[i] == -1:scatter_data.append(go.Scatter(x=[x[i]], y=[0], mode='markers', marker=dict(size=10),name=f\"Cluster {x[i]}\"))\n",
    "        else: bar_data.append(go.Bar(x=[x[i]], y=[y[i]],name=f\"Cluster {x[i]}\"))\n",
    "\n",
    "    fig = go.Figure(data=bar_data + scatter_data)\n",
    "\n",
    "    fig.update_layout(xaxis_title_text=\"Cluster ID\")\n",
    "    fig.update_layout(yaxis_title_text=\"Variance\")\n",
    "    fig.update_layout(title_text=\"Graph Showing Variance in Each Cluster\")\n",
    "    fig.update_layout(xaxis_dtick=1, xaxis_range=[0, max(x)])\n",
    "    fig.show()\n",
    "\n",
    "def show_community_member_counts(community):\n",
    "    clusters=get_clusters(community)\n",
    "    cluster_counts = {}\n",
    "    for key in clusters.keys():\n",
    "        cluster_counts[key] = len(clusters[key])\n",
    "    \n",
    "    fig = go.Figure(data=[go.Bar(x=list(cluster_counts.keys()), y=list(cluster_counts.values()))])\n",
    "    fig.update_layout(\n",
    "                        title=\"Graph Showing Node Count in Each Cluster\",\n",
    "                        xaxis_title=\"Cluster ID\",\n",
    "                        yaxis_title=\"Count\"\n",
    "                        )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cdlib import algorithms\n",
    "\n",
    "x = algorithms.surprise_communities\n",
    "def make_communities(alg,G):    \n",
    "    print(f\"creating community\")\n",
    "    community = eval(f\"algorithms.{alg}(G)\")\n",
    "    print(f\"community created\")\n",
    "    analyze_clusters(community)\n",
    "    show_community_member_counts(community)\n",
    "\n",
    "    return community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import util\n",
    "\n",
    "def get_embeddings_from_community(contexts,community):\n",
    "    embeddings = []\n",
    "    for node in community:\n",
    "        text_node = contexts[str(node)]\n",
    "        embeddings.append(text_node.embedding)\n",
    "    embeddings_tensor = torch.tensor(embeddings).to('cuda')\n",
    "    \n",
    "    return embeddings_tensor\n",
    "\n",
    "def get_similarity_matrix(tensors):\n",
    "    similarity_matrix = util.cos_sim(tensors, tensors)\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "def create_community_graph(community,similarity_matrix):\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(community)\n",
    "\n",
    "    for i in range(len(community)):\n",
    "        for j in range(i+1, len(community)):\n",
    "            similarity = similarity_matrix[i][j]\n",
    "    \n",
    "            G.add_edge(community[i], community[j], weight=similarity.item())\n",
    "    return G\n",
    "\n",
    "# embeddings = get_embeddings_from_community(contexts=contexts,community=clusters[0])\n",
    "# embeddings_tensor = torch.tensor(embeddings).to('cuda')\n",
    "# sim_mat = get_similarity_matrix(embeddings_tensor)\n",
    "# comm_graph  = create_community_graph(clusters[0],sim_mat)\n",
    "# graph_db.visualize_graph(comm_graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Louvain Community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "louvain_comm = make_communities('louvain',G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leiden Community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leiden_comm = make_communities('leiden',G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surprise Community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprise_comm = make_communities('surprise_communities',G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walktrap Community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walktrap_comm = make_communities('walktrap',G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_num_communities(community)\n",
    "draw_graph(G, pos, community.communities)\n",
    "analyze_clusters(community)\n",
    "louvain_cluster_counts = get_community_member_counts(community)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation_metrics.retriever import RetrieverEvaluator\n",
    "\n",
    "evaluator = RetrieverEvaluator(hotpot_qa_df,'a_star_retrieved')\n",
    "\n",
    "order_unaware_metrics = evaluator.get_order_unaware_metrics(k=1) \n",
    "#order_aware_metrics = evaluator.get_order_aware_metrics() \n",
    "\n",
    "print(order_unaware_metrics)\n",
    "#print(order_aware_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25 = create_retriever(embedded_chunks,10)\n",
    "df_results_bm25 = retrieve_chunks_bm25(BM25,embedded_queries)\n",
    "evaluate_results(df_results_bm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.5\n",
    "#G = graph_db.create_graph(embedded_chunks,chunk_similarities,THRESHOLD,save_dir)\n",
    "df_results = retrieve_chunk_graph(db=G,\n",
    "                                  traversal_method='a star',\n",
    "                                  threshold=THRESHOLD,\n",
    "                                  query_bundles=embedded_queries)\n",
    "evaluate_results(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
