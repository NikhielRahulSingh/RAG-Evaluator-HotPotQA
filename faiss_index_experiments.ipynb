{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "from utils.hotpot_data_loader import load_test_data\n",
    "\n",
    "model_name=\"mixedbread-ai/mxbai-embed-large-v1\"\n",
    "\n",
    "query_embeddings = numpy.load(f\"test/{model_name}/query_embeddings.npy\")\n",
    "context_embeddings = numpy.load(f\"test/{model_name}/context_embeddings.npy\")\n",
    "\n",
    "questions,contexts,benchmarks = load_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recall@10   : 1.00\n",
      "Precision@10: 1.00\n",
      "\n",
      "NDCG@10     : 1.00\n",
      "MAP@10      : 1.00\n",
      "MRR@10      : 1.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from utils.evaluation_metrics.retriever import RetrieverEvaluator\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "def retrieve_contexts(index):\n",
    "    retrieved_results = [] \n",
    "    for query_embedding in tqdm(query_embeddings):\n",
    "        #D, I = index.search(np.array(query_embedding).reshape(1, -1), 10)\n",
    "        D, I = index.search(query_embedding.reshape(1, -1), 10)\n",
    "        results = I[0].tolist()\n",
    "        retrieved_results.append(results)\n",
    "    return retrieved_results\n",
    "\n",
    "\n",
    "\n",
    "save_dir = f\"{f\"test_results/{model_name}/\"}\"\n",
    "if not os.path.exists(save_dir):os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flat Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inner Product (IP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7405/7405 [01:37<00:00, 75.87it/s]\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "index = faiss.IndexFlatIP(len(query_embeddings[0]))\n",
    "index.add(context_embeddings)\n",
    "\n",
    "retrieved_contexts = retrieve_contexts(index)\n",
    "\n",
    "with open(f\"{save_dir}/index_flat_ip.pkl\", 'wb') as file: pickle.dump(retrieved_contexts, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7405/7405 [01:43<00:00, 71.87it/s]\n"
     ]
    }
   ],
   "source": [
    "index_l2 = faiss.IndexFlatL2(len(query_embeddings[0]))\n",
    "index_l2.add(context_embeddings)\n",
    "\n",
    "retrieved_contexts = retrieve_contexts(index_l2)\n",
    "\n",
    "with open(f\"{save_dir}/index_flat_l2.pkl\", 'wb') as file: pickle.dump(retrieved_contexts, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Navigable Small World "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7405/7405 [00:11<00:00, 617.75it/s]\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "max_neighbours = 10 # maximum number of neighbour connections a vector can have\n",
    "search_ef = 1000 # number of neighbours in the HNSW graph to explore when searching.\n",
    "search_ef = 500 # number of neighbours in the HNSW graph to explore when adding new vectors. \n",
    "\n",
    "index = faiss.IndexHNSWFlat(len(query_embeddings[0]), max_neighbours)\n",
    "index.hnsw.efSearch = search_ef\n",
    "index.hnsw.efConstruction = search_ef\n",
    "\n",
    "index.add(context_embeddings)\n",
    "\n",
    "retrieved_contexts = retrieve_contexts(index)\n",
    "with open(f\"{save_dir}/index_hnsw.pkl\", 'wb') as file: pickle.dump(retrieved_contexts, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverted File System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7405/7405 [00:07<00:00, 960.01it/s] \n"
     ]
    }
   ],
   "source": [
    "nlist = 5000  # how many voronoi cells\n",
    "nprobe = 50 # how many nearby voronoi cells to search\n",
    "\n",
    "quantizer = faiss.IndexFlatIP(len(query_embeddings[0]))\n",
    "index_ivf = faiss.IndexIVFFlat(quantizer, len(query_embeddings[0]), nlist)\n",
    "index_ivf.train(context_embeddings)\n",
    "index_ivf.add(context_embeddings)\n",
    "index_ivf.nprobe = nprobe\n",
    "\n",
    "retrieved_contexts = retrieve_contexts(index_ivf)\n",
    "with open(f\"{save_dir}/index_ivf.pkl\", 'wb') as file: pickle.dump(retrieved_contexts, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical (BMX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 73774/73774 [00:17<00:00, 4220.13it/s]\n",
      "Building doc-term matrix: 100%|██████████| 73774/73774 [00:01<00:00, 43249.97it/s]\n",
      "Building inverted index: 100%|██████████| 147615/147615 [00:24<00:00, 6088.05it/s]\n",
      "100%|██████████| 7405/7405 [00:09<00:00, 744.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from baguetter.indices import *\n",
    "\n",
    "index_lexical =  BMXSparseIndex(index_name=\"BMX_Test\")\n",
    "keys = {i: i + 1 for i in range(len(context_embeddings))}\n",
    "index_lexical.add_many(keys=keys,\n",
    "                       values=[f\"Title: {doc.metadata[\"Title\"]}\\nExtract: {doc.page_content}\" for doc in contexts],\n",
    "                       show_progress=True)\n",
    "\n",
    "retrieved_contexts = []\n",
    "\n",
    "for question in tqdm(questions):\n",
    "    x = index_lexical.search(query=question,top_k=10*2)\n",
    "\n",
    "    keys = x.keys[:10]\n",
    "    retrieved_contexts.append(x.keys)\n",
    "\n",
    "with open(f\"{save_dir}/index_bmx.pkl\", 'wb') as file: pickle.dump(retrieved_contexts, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"WhereIsAI/UAE-Large-V1\"\n",
    "model_kwargs = {'device': 'cuda',\n",
    "                'trust_remote_code':True,}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=f\"D:/Users/nikhi/hugging_face_embedding_models/{model_name}\",\n",
    "    model_kwargs=model_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "contexts = [f\"Title: {doc.metadata[\"Title\"]}\\nExtract: {doc.page_content}\" for doc in contexts]\n",
    "metadatas = [{\"id\":i} for i in range(len(contexts))]\n",
    "bm25_retriever = BM25Retriever.from_texts(\n",
    "    contexts, metadatas=metadatas\n",
    ")\n",
    "bm25_retriever.k = 10\n",
    "\n",
    "kwargs = {\"embedding_function\":hf}\n",
    "text_embedding_pairs = zip(contexts, context_embeddings)\n",
    "faiss_vector_store = FAISS.from_embeddings(text_embedding_pairs,context_embeddings)\n",
    "faiss_vector_store.embedding_function = hf\n",
    "faiss_retriever = faiss_vector_store.as_retriever(search_kwargs={\"k\": 10})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever,faiss_retriever], \n",
    "    weights=[0.5, 0.5],\n",
    ")\n",
    "\n",
    "@chain\n",
    "def search_by_vector(vector: list[float]) -> list[Document]:\n",
    "    return ensemble_retriever.similarity_search_by_vector(vector)\n",
    "\n",
    "x = search_by_vector.invoke(query_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def ensemble_retrieval(question):\n",
    "    x = ensemble_retriever.invoke(question)\n",
    "    retrieved_contexts = []\n",
    "\n",
    "    for _ in x:\n",
    "        retireved = []\n",
    "        try:\n",
    "            id = _.metadata[\"id\"]\n",
    "            retrieved_contexts.append(id)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return retrieved_contexts\n",
    "\n",
    "\n",
    "contexts = []\n",
    "for query in tqdm(questions, desc=\"Processing queries\"):\n",
    "    result = ensemble_retrieval(query)\n",
    "    contexts.append(result)\n",
    "\n",
    "beir_evaluation(contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 73774/73774 [00:16<00:00, 4494.27it/s] \n",
      "Building doc-term matrix: 100%|██████████| 73774/73774 [00:01<00:00, 46154.01it/s]\n",
      "Building inverted index: 100%|██████████| 147615/147615 [00:23<00:00, 6159.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semantic and lexical indices created\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7405/7405 [00:09<00:00, 806.40it/s]\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy\n",
    "from tqdm import tqdm\n",
    "from baguetter.indices import *\n",
    "\n",
    "query_embeddings = numpy.load(f\"test/{model_name}/query_embeddings.npy\")\n",
    "context_embeddings = numpy.load(f\"test/{model_name}/context_embeddings.npy\")\n",
    "\n",
    "questions,contexts,benchmarks = load_test_data()\n",
    "\n",
    "class HybridSearch:\n",
    "    def __init__(self, \n",
    "                 top_n,\n",
    "                 contexts=contexts,\n",
    "                 context_embeddings=context_embeddings,\n",
    "                 questions=questions,\n",
    "                 query_embeddings=query_embeddings):\n",
    "        \n",
    "        self.top_n = top_n\n",
    "        self.contexts = contexts\n",
    "        self.context_embeddings = context_embeddings\n",
    "        self.questions = questions\n",
    "        self.query_embeddings = query_embeddings\n",
    "        \n",
    "    def create_lexical_index(self,contexts):\n",
    "\n",
    "        index_lexical =  BMXSparseIndex(index_name=\"BMX_Test\")\n",
    "        keys = {i: i + 1 for i in range(len(contexts))}\n",
    "        index_lexical.add_many(keys=keys,\n",
    "                            values=[f\"Title: {doc.metadata[\"Title\"]}\\nExtract: {doc.page_content}\" for doc in contexts],\n",
    "                            show_progress=True)\n",
    "        \n",
    "        return index_lexical\n",
    "\n",
    "    def create_semantic_index(self,context_embeddings):\n",
    "        \n",
    "        semantic_index = faiss.IndexFlatIP(len(context_embeddings[0]))\n",
    "        semantic_index.add(context_embeddings)\n",
    "\n",
    "        return semantic_index\n",
    "        \n",
    "    def lexical_then_semantic(self,query_num:int):\n",
    "\n",
    "        lexical_index = self.lexical_index\n",
    "        lexical_retrived_docs = lexical_index.search(query=self.questions[query_num],top_k=self.top_n*2)\n",
    "        lexical_retrieved_embeddings = numpy.array([self.context_embeddings[i] for i in list(lexical_retrived_docs.keys)])\n",
    "\n",
    "        semantic_index = self.create_semantic_index(lexical_retrieved_embeddings)\n",
    "        D, I = semantic_index.search(self.query_embeddings[query_num].reshape(1, -1), self.top_n)\n",
    "        semantic_retrieved_docs = I[0].tolist()\n",
    "        _ = [lexical_retrived_docs.keys[i] for i in semantic_retrieved_docs]\n",
    "\n",
    "        return _\n",
    "        \n",
    "    def retrieve_contexts(self):\n",
    "\n",
    "        self.lexical_index = self.create_lexical_index(self.contexts)\n",
    "        self.semantic_index = self.create_semantic_index(self.context_embeddings)\n",
    "        print(f\"semantic and lexical indices created\\n\")\n",
    "\n",
    "        retrieved_results = [] \n",
    "        for query_num in tqdm(range(len(questions))):\n",
    "            retrieved_results.append(self.lexical_then_semantic(query_num))\n",
    "            \n",
    "        return retrieved_results\n",
    "\n",
    "hs = HybridSearch(top_n=10)\n",
    "results = hs.retrieve_contexts()\n",
    "with open(f\"{save_dir}/index_hybrid.pkl\", 'wb') as file: pickle.dump(retrieved_contexts, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
