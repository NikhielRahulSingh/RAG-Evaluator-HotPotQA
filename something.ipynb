{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'clustering/graph/connected_graph.pkl', 'rb') as file: connected_graph = pickle.load(file)\n",
    "with open(f'clustering/graph/cluster_graph.pkl', 'rb') as file: cluster_graph = pickle.load(file)\n",
    "with open(f'embeddings/dunzhang/stella_en_400M_v5/hard/500/df.pkl', 'rb') as file: hotpot_qa_df = pickle.load(file)\n",
    "with open(f'embeddings/dunzhang/stella_en_400M_v5/hard/500/contexts.pkl', 'rb') as file: contexts = pickle.load(file)\n",
    "\n",
    "hotpot_qa_df['actual_contexts'] = hotpot_qa_df['actual_contexts'].apply(lambda x: [int(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict,List\n",
    "import networkx\n",
    "import random\n",
    "\n",
    "cluster_node_embedding_sample:Dict[int,List[float]] = {}\n",
    "for cluster_id in cluster_graph.keys():\n",
    "    graph:networkx.Graph = cluster_graph[cluster_id]\n",
    "    node:int = list(graph.nodes)[0]#random.choice(list(graph.nodes)) # select a random node from the cluster\n",
    "    cluster_node_embedding_sample[cluster_id] = contexts[str(node)].embedding\n",
    "\n",
    "start_node = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A* Traversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def cosine_similarity_gpu(list_A, list_B):\n",
    "    # Convert lists to tensors and move to GPU\n",
    "    A = torch.tensor(list_A, dtype=torch.float32).cuda()\n",
    "    B = torch.tensor(list_B, dtype=torch.float32).cuda()\n",
    "\n",
    "    # Compute cosine similarity on GPU\n",
    "    cos_sim = F.cosine_similarity(A.unsqueeze(0), B.unsqueeze(0))\n",
    "\n",
    "    return cos_sim.item()  # Convert tensor result to a Python float\n",
    "\n",
    "def get_nodes(question_embedding,clusters,contexts=contexts):\n",
    "    cosine_similarities = []\n",
    "    nodes = []\n",
    "    for cluster in clusters:\n",
    "        for node in list(cluster_graph[cluster].nodes):\n",
    "            nodes.append(node)\n",
    "\n",
    "    # Calculate cosine similarities and store them with node identifiers\n",
    "    for node in nodes:\n",
    "        node_embedding = contexts[str(node)].embedding\n",
    "        cosine_sim = cosine_similarity_gpu(question_embedding, node_embedding)\n",
    "        cosine_similarities.append((node, cosine_sim))  # Store as (node, similarity)\n",
    "\n",
    "    # Sort the nodes based on the highest similarity\n",
    "    sorted_nodes = sorted(cosine_similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Extract sorted node identifiers and their similarities\n",
    "    sorted_node_ids = [node for node, sim in sorted_nodes]\n",
    "    sorted_similarities = [sim for node, sim in sorted_nodes]\n",
    "\n",
    "    return sorted_node_ids[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving contexts: 100%|██████████| 500/500 [10:52<00:00,  1.31s/question]\n"
     ]
    }
   ],
   "source": [
    "THRESHOLD = 0.50\n",
    "\n",
    "retrieved_contexts = []\n",
    "for question in tqdm(hotpot_qa_df['question'], desc=\"Retrieving contexts\", unit=\"question\"):\n",
    "\n",
    "    question_embedding = question.embedding\n",
    "\n",
    "    max_cluster_similarities:List[int] = []\n",
    "    max_similarity:float = -float('inf')\n",
    "    max_similirity_node:int = None\n",
    "\n",
    "    for id,sample_embedding in zip(cluster_node_embedding_sample.keys(),cluster_node_embedding_sample.values()):\n",
    "        cosine_sim = cosine_similarity_gpu(question_embedding,sample_embedding)\n",
    "        if cosine_sim > max_similarity: \n",
    "            max_similarity = cosine_sim\n",
    "            max_similirity_node = id\n",
    "        if cosine_sim > THRESHOLD:max_cluster_similarities.append(id)\n",
    "\n",
    "    if len(max_cluster_similarities) == 0: \n",
    "        max_cluster_similarities.append(max_similirity_node)\n",
    "    \n",
    "    retrieved_contexts.append(get_nodes(question_embedding,max_cluster_similarities))\n",
    "\n",
    "hotpot_qa_df[\"Cluster_Retrieval\"] = retrieved_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving contexts: 100%|██████████| 500/500 [00:00<00:00, 580.53question/s]\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "\n",
    "documents = []\n",
    "ids = []\n",
    "embeddings = []\n",
    "metadatas = []\n",
    "\n",
    "for context in contexts.values():\n",
    "    documents.append(context.text)\n",
    "    ids.append(context.id_)\n",
    "    embeddings.append(context.embedding)\n",
    "    metadatas.append({'caption': context.metadata['caption']})\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(name=\"my_collection2\",metadata={\"hnsw:space\": \"cosine\"})\n",
    "collection.add(documents=documents,\n",
    "               ids=ids,\n",
    "               embeddings=embeddings,\n",
    "               metadatas=metadatas)\n",
    "\n",
    "retrieved_contexts = []\n",
    "for question in tqdm(hotpot_qa_df['question'], desc=\"Retrieving contexts\", unit=\"question\"):\n",
    "\n",
    "    question_embedding = question.embedding\n",
    "    result = collection.query(\n",
    "                                query_embeddings=[question_embedding], # Chroma will embed this for you\n",
    "                                n_results=10 # how many results to return\n",
    "                              )\n",
    "    retrieved_contexts.append([int(node) for node in result[\"ids\"][0]] )\n",
    "\n",
    "hotpot_qa_df[\"Vector_Similarity_Retrieval\"] = retrieved_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster_Retrieval:\n",
      "order unaware metrics : {'avg precision@10': 0.461, 'avg recall@10': 0.95, 'avg F1@10': 0.5770785160831601}\n",
      "order aware metrics   : {'avg mrr': 0.5007951284958427, 'avg ndcg': 0.5650973212401913, 'mean avg precision': 0.44296857142857143}\n"
     ]
    }
   ],
   "source": [
    "from utils.evaluation_metrics.retriever import RetrieverEvaluator\n",
    "\n",
    "evaluator = RetrieverEvaluator(hotpot_qa_df,'Cluster_Retrieval')\n",
    "\n",
    "order_unaware_metrics = evaluator.get_order_unaware_metrics(k=10) \n",
    "order_aware_metrics = evaluator.get_order_aware_metrics() \n",
    "\n",
    "print(f\"Cluster_Retrieval:\")\n",
    "print(f\"order unaware metrics : {order_unaware_metrics}\")\n",
    "print(f\"order aware metrics   : {order_aware_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector_Similarity_Retrieval:\n",
      "order unaware metrics : {'avg precision@10': 0.5963999999999999, 'avg recall@10': 0.992, 'avg F1@10': 0.7160102046216597}\n",
      "order aware metrics   : {'avg mrr': 0.41551405549256737, 'avg ndcg': 0.6828619205100309, 'mean avg precision': 0.5532635714285714}\n"
     ]
    }
   ],
   "source": [
    "evaluator = RetrieverEvaluator(hotpot_qa_df,'Vector_Similarity_Retrieval')\n",
    "\n",
    "order_unaware_metrics = evaluator.get_order_unaware_metrics(k=10) \n",
    "order_aware_metrics = evaluator.get_order_aware_metrics() \n",
    "\n",
    "print(f\"Vector_Similarity_Retrieval:\")\n",
    "print(f\"order unaware metrics : {order_unaware_metrics}\")\n",
    "print(f\"order aware metrics   : {order_aware_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = []\n",
    "ids = []\n",
    "embeddings = []\n",
    "metadatas = []\n",
    "\n",
    "for context in contexts.values():\n",
    "    documents.append(Document(page_content=context.text, metadata={'caption': context.metadata['caption']}))\n",
    "    ids.append(context.id_)\n",
    "    embeddings.append(context.embedding)\n",
    "    metadatas.append({'caption': context.metadata['caption']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"dunzhang/stella_en_400M_v5\"\n",
    "model_kwargs = {\n",
    "    \"trust_remote_code\": True,\n",
    "    \"device\": \"cuda\",\n",
    "    \"model_kwargs\": {\"attn_implementation\": \"eager\"}\n",
    "}\n",
    "\n",
    "model = HuggingFaceEmbeddings(show_progress=True,\n",
    "                              model_name=\"D:/Users/nikhi/hugging_face_embedding_models/dunzhang/stella_en_400M_v5\",\n",
    "                              model_kwargs = model_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement scann (from versions: none)\n",
      "ERROR: No matching distribution found for scann\n"
     ]
    }
   ],
   "source": [
    "!pip install scann\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c209bf67c9ea4542b90a5cb65fbc70b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "Could not import scann python package. Please install it with `pip install scann`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32md:\\Users\\nikhi\\anaconda3\\envs\\masters\\Lib\\site-packages\\langchain_core\\utils\\utils.py:135\u001b[0m, in \u001b[0;36mguard_import\u001b[1;34m(module_name, pip_name, package)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Users\\nikhi\\anaconda3\\envs\\masters\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1324\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'scann'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ScaNN\n\u001b[1;32m----> 3\u001b[0m db \u001b[38;5;241m=\u001b[39m \u001b[43mScaNN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\nikhi\\anaconda3\\envs\\masters\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:852\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[0;32m    850\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ids\n\u001b[1;32m--> 852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\nikhi\\anaconda3\\envs\\masters\\Lib\\site-packages\\langchain_community\\vectorstores\\scann.py:386\u001b[0m, in \u001b[0;36mScaNN.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct ScaNN wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \n\u001b[0;32m    370\u001b[0m \u001b[38;5;124;03mThis is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;124;03m        scann = ScaNN.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    385\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39membed_documents(texts)\n\u001b[1;32m--> 386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__from\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\nikhi\\anaconda3\\envs\\masters\\Lib\\site-packages\\langchain_community\\vectorstores\\scann.py:310\u001b[0m, in \u001b[0;36mScaNN.__from\u001b[1;34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, **kwargs)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__from\u001b[39m(\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    309\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ScaNN:\n\u001b[1;32m--> 310\u001b[0m     scann \u001b[38;5;241m=\u001b[39m \u001b[43mguard_import\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscann\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m     distance_strategy \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    312\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance_strategy\u001b[39m\u001b[38;5;124m\"\u001b[39m, DistanceStrategy\u001b[38;5;241m.\u001b[39mEUCLIDEAN_DISTANCE\n\u001b[0;32m    313\u001b[0m     )\n\u001b[0;32m    314\u001b[0m     scann_config \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscann_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Users\\nikhi\\anaconda3\\envs\\masters\\Lib\\site-packages\\langchain_core\\utils\\utils.py:142\u001b[0m, in \u001b[0;36mguard_import\u001b[1;34m(module_name, pip_name, package)\u001b[0m\n\u001b[0;32m    137\u001b[0m     pip_name \u001b[38;5;241m=\u001b[39m pip_name \u001b[38;5;129;01mor\u001b[39;00m module_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    138\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    141\u001b[0m     )\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "\u001b[1;31mImportError\u001b[0m: Could not import scann python package. Please install it with `pip install scann`."
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import ScaNN\n",
    "\n",
    "db = ScaNN.from_documents(documents=documents[:3], embedding=model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
