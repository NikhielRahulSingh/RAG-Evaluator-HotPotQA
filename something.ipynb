{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model = \"dunzhang/stella_en_400M_v5\"\n",
    "\n",
    "with open(f'clustering/graph/connected_graph.pkl', 'rb') as file: connected_graph = pickle.load(file)\n",
    "with open(f'clustering/graph/cluster_graph.pkl', 'rb') as file: cluster_graph = pickle.load(file)\n",
    "with open(f'embeddings/{model}/hard/3000/df.pkl', 'rb') as file: hotpot_qa_df = pickle.load(file)\n",
    "with open(f'embeddings/{model}/hard/3000/contexts.pkl', 'rb') as file: contexts = pickle.load(file)\n",
    "\n",
    "hotpot_qa_df['actual_contexts'] = hotpot_qa_df['actual_contexts'].apply(lambda x: [int(i) for i in x])\n",
    "hotpot_qa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict,List\n",
    "import networkx\n",
    "import random\n",
    "\n",
    "cluster_node_embedding_sample:Dict[int,List[float]] = {}\n",
    "for cluster_id in cluster_graph.keys():\n",
    "    graph:networkx.Graph = cluster_graph[cluster_id]\n",
    "    node:int = list(graph.nodes)[0]#random.choice(list(graph.nodes)) # select a random node from the cluster\n",
    "    cluster_node_embedding_sample[cluster_id] = contexts[str(node)].embedding\n",
    "\n",
    "start_node = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A* Traversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def cosine_similarity_gpu(list_A, list_B):\n",
    "    # Convert lists to tensors and move to GPU\n",
    "    A = torch.tensor(list_A, dtype=torch.float32).cuda()\n",
    "    B = torch.tensor(list_B, dtype=torch.float32).cuda()\n",
    "\n",
    "    # Compute cosine similarity on GPU\n",
    "    cos_sim = F.cosine_similarity(A.unsqueeze(0), B.unsqueeze(0))\n",
    "\n",
    "    return cos_sim.item()  # Convert tensor result to a Python float\n",
    "\n",
    "def get_nodes(question_embedding,clusters,contexts=contexts):\n",
    "    cosine_similarities = []\n",
    "    nodes = []\n",
    "    for cluster in clusters:\n",
    "        for node in list(cluster_graph[cluster].nodes):\n",
    "            nodes.append(node)\n",
    "\n",
    "    # Calculate cosine similarities and store them with node identifiers\n",
    "    for node in nodes:\n",
    "        node_embedding = contexts[str(node)].embedding\n",
    "        cosine_sim = cosine_similarity_gpu(question_embedding, node_embedding)\n",
    "        cosine_similarities.append((node, cosine_sim))  # Store as (node, similarity)\n",
    "\n",
    "    # Sort the nodes based on the highest similarity\n",
    "    sorted_nodes = sorted(cosine_similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Extract sorted node identifiers and their similarities\n",
    "    sorted_node_ids = [node for node, sim in sorted_nodes]\n",
    "    sorted_similarities = [sim for node, sim in sorted_nodes]\n",
    "\n",
    "    return sorted_node_ids[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.60\n",
    "\n",
    "retrieved_contexts = []\n",
    "for question in tqdm(hotpot_qa_df['question'], desc=\"Retrieving contexts\", unit=\"question\"):\n",
    "\n",
    "    question_embedding = question.embedding\n",
    "\n",
    "    max_cluster_similarities:List[int] = []\n",
    "    max_similarity:float = -float('inf')\n",
    "    max_similirity_node:int = None\n",
    "\n",
    "    for id,sample_embedding in zip(cluster_node_embedding_sample.keys(),cluster_node_embedding_sample.values()):\n",
    "        cosine_sim = cosine_similarity_gpu(question_embedding,sample_embedding)\n",
    "        if cosine_sim > max_similarity: \n",
    "            max_similarity = cosine_sim\n",
    "            max_similirity_node = id\n",
    "        if cosine_sim > THRESHOLD:max_cluster_similarities.append(id)\n",
    "\n",
    "    if len(max_cluster_similarities) == 0: \n",
    "        max_cluster_similarities.append(max_similirity_node)\n",
    "    \n",
    "    retrieved_contexts.append(get_nodes(question_embedding,max_cluster_similarities))\n",
    "\n",
    "hotpot_qa_df[\"Cluster_Retrieval\"] = retrieved_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_index(retrieved_contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChromaDB Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "\n",
    "documents = []\n",
    "ids = []\n",
    "embeddings = []\n",
    "metadatas = []\n",
    "\n",
    "for context in contexts.values():\n",
    "    documents.append(context.text)\n",
    "    ids.append(context.id_)\n",
    "    embeddings.append(context.embedding)\n",
    "    metadatas.append({'caption': context.metadata['caption']})\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(name=\"my_collection\",metadata={\"hnsw:space\": \"cosine\"})\n",
    "collection.add(documents=documents,\n",
    "               ids=ids,\n",
    "               embeddings=embeddings,\n",
    "               metadatas=metadatas)\n",
    "\n",
    "retrieved_contexts = []\n",
    "for question in tqdm(hotpot_qa_df['question'], desc=\"Retrieving contexts\", unit=\"question\"):\n",
    "\n",
    "    question_embedding = question.embedding\n",
    "    result = collection.query(\n",
    "                                query_embeddings=[question_embedding], # Chroma will embed this for you\n",
    "                                n_results=10 # how many results to return\n",
    "                              )\n",
    "    retrieved_contexts.append([int(node) for node in result[\"ids\"][0]] )\n",
    "\n",
    "hotpot_qa_df[\"Vector_Similarity_Retrieval\"] = retrieved_contexts\n",
    "\n",
    "\n",
    "from utils.evaluation_metrics.retriever import RetrieverEvaluator\n",
    "\n",
    "evaluator = RetrieverEvaluator(hotpot_qa_df,'Vector_Similarity_Retrieval')\n",
    "\n",
    "order_unaware_metrics = evaluator.get_order_unaware_metrics(k=10) \n",
    "order_aware_metrics = evaluator.get_order_aware_metrics() \n",
    "\n",
    "print(f\"Cluster_Retrieval:\")\n",
    "print(f\"order unaware metrics : {order_unaware_metrics}\")\n",
    "print(f\"order aware metrics   : {order_aware_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.evaluation_metrics.retriever import RetrieverEvaluator\n",
    "\n",
    "context_embeddings = []\n",
    "\n",
    "for text_node in contexts.values():\n",
    "    context_embeddings.append(text_node.embedding)\n",
    "    \n",
    "def lists_to_arrays(list_of_lists):\n",
    "    return np.array([np.array(lst) for lst in list_of_lists], dtype=object)\n",
    "\n",
    "query_embeddings = [query.embedding for query in hotpot_qa_df['question'].tolist()]\n",
    "\n",
    "def retrieve_contexts(index):\n",
    "    retrieved_results = [] \n",
    "    for query_embedding in tqdm(query_embeddings):\n",
    "        D, I = index.search(np.array(query_embedding).reshape(1, -1), 10)\n",
    "        results = I[0].tolist()\n",
    "        retrieved_results.append(results)\n",
    "    return retrieved_results\n",
    "\n",
    "actual_contexts = hotpot_qa_df['actual_contexts'].tolist()\n",
    "def evaluate_index(retrieved_contexts,actual_contexts=actual_contexts):\n",
    "    df = pd.DataFrame({\n",
    "                        'actual_contexts': actual_contexts,\n",
    "                        'retrieved_contexts': retrieved_contexts\n",
    "                    })\n",
    "    evaluator = RetrieverEvaluator(df,'retrieved_contexts')\n",
    "    order_unaware_metrics = evaluator.get_order_unaware_metrics(k=10) \n",
    "    order_aware_metrics = evaluator.get_order_aware_metrics() \n",
    "\n",
    "    print(f\"order unaware metrics : {order_unaware_metrics}\")\n",
    "    print(f\"order aware metrics   : {order_aware_metrics}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flat Methods : IP & L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Product (IP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatIP(len(query_embeddings[0]))\n",
    "index.add(lists_to_arrays(context_embeddings))\n",
    "\n",
    "contexts = retrieve_contexts(index)\n",
    "evaluate_index(contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean (L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation_metrics.retriever import RetrieverEvaluator\n",
    "\n",
    "index = faiss.IndexFlatL2(len(query_embeddings[0]))\n",
    "index.add(lists_to_arrays(context_embeddings))\n",
    "\n",
    "retrieved_contexts = retrieve_contexts(index)\n",
    "evaluate_index(retrieved_contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Navigable Small World "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation_metrics.retriever import RetrieverEvaluator\n",
    "\n",
    "max_neighbours = 16\n",
    "ef_search = 10\n",
    "ef_construction = 256\n",
    "\n",
    "index = faiss.IndexHNSWFlat(len(query_embeddings[0]), max_neighbours)\n",
    "index.hnsw.efSearch = ef_search\n",
    "index.hnsw.efConstruction = ef_construction\n",
    "\n",
    "index.add(lists_to_arrays(context_embeddings))\n",
    "\n",
    "contexts = retrieve_contexts(index)\n",
    "evaluate_index(contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25 then IndexFlat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "class HybridSearch:\n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "\n",
    "        # BM25 initialization\n",
    "        tokenized_corpus = [text_node.text.split(\" \") for text_node in documents]\n",
    "        self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "        self.document_embeddings = [text_node.embedding for text_node in documents]\n",
    "        \n",
    "        # FAISS initialization\n",
    "        self.index = faiss.IndexFlatIP(len(self.document_embeddings[0]))\n",
    "        self.index.add(lists_to_arrays(self.document_embeddings))\n",
    "\n",
    "    def search(self, query, top_n=10):\n",
    "        # BM25 search\n",
    "        bm25_scores = self.bm25.get_scores(query.query_str.split(\" \"))\n",
    "        top_docs_indices = np.argsort(bm25_scores)[-top_n*5:]\n",
    "        print(top_docs_indices)\n",
    "        print()\n",
    "        \n",
    "        # Get embeddings of top documents from BM25 search\n",
    "        top_docs_embeddings = [self.document_embeddings[i] for i in top_docs_indices]\n",
    "\n",
    "        query_embedding = np.array(query.embedding).reshape(1, -1)\n",
    "\n",
    "        # FAISS search on the top documents\n",
    "        sub_index = faiss.IndexFlatIP(len(self.document_embeddings[0]))\n",
    "        sub_index.add(np.array(top_docs_embeddings))\n",
    "        distances, sub_dense_ranked_indices = sub_index.search(np.array(query_embedding), top_n)\n",
    "\n",
    "        # Map FAISS results back to original document indices\n",
    "        final_ranked_indices = [top_docs_indices[i] for i in sub_dense_ranked_indices[0]]\n",
    "\n",
    "        # Retrieve the actual documents\n",
    "        ranked_docs = [int(self.documents[i].id_) for i in final_ranked_indices]\n",
    "        print(ranked_docs)\n",
    "        print()\n",
    "        print()\n",
    "        return ranked_docs\n",
    "\n",
    "def retrieve_contexts(hs):\n",
    "    retrieved_results = [] \n",
    "    questions = hotpot_qa_df[\"question\"].tolist()\n",
    "    for query in tqdm(questions[:2]):\n",
    "        retrieved_results.append(hs.search(query, top_n=10))\n",
    "    return retrieved_results\n",
    "\n",
    "hs = HybridSearch(list(contexts.values()))\n",
    "results = retrieve_contexts(hs)\n",
    "evaluate_index(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baguetter.indices import FaissDenseIndex,BMXSparseIndex,MultiIndex\n",
    "import pickle\n",
    "\n",
    "def load_data(model_name):\n",
    "    with open(f'embeddings/{model_name}/hard/3000/df.pkl', 'rb') as file: hotpot_qa_df = pickle.load(file)\n",
    "    with open(f'embeddings/{model_name}/hard/3000/contexts.pkl', 'rb') as file: contexts = pickle.load(file)\n",
    "\n",
    "    questions = hotpot_qa_df[\"question\"].tolist()\n",
    "\n",
    "\n",
    "    return [q.embedding for q in questions],contexts\n",
    "\n",
    "dense_model = \"dunzhang/stella_en_400M_v5\"\n",
    "dense_questions,dense_contexts = load_data(dense_model)\n",
    "result = {i: i + 1 for i in range(len(context_embeddings))}\n",
    "# Create an index\n",
    "dense_index = FaissDenseIndex(index,\"dense_index\",len(query_embeddings[0]),result)\n",
    "\n",
    "_ = dense_index.search(np.array(dense_questions[0]),top_k=10)\n",
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baguetter.indices import *\n",
    "\n",
    "\n",
    "context_str = [x.text for x in list(dense_contexts.values())]\n",
    "sparse_index = BMXSparseIndex(index_name=\"BMX_Test\")\n",
    "sparse_index.add_many(keys=result,values=context_str,show_progress=True)\n",
    "\n",
    "questions = hotpot_qa_df[\"question\"].tolist()\n",
    "question_str = [q.query_str for q in questions]\n",
    "x = sparse_index.search(question_str[0],top_k=10)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_index = MultiIndex()\n",
    "\n",
    "idx = multi_index.add_index(sparse_index)\n",
    "#idx = multi_index.add_index(dense_index)\n",
    "\n",
    "embeddings = [q.embedding for q in hotpot_qa_df[\"question\"].tolist()]\n",
    "question_strs = [q.query_str for q in hotpot_qa_df[\"question\"].tolist()]\n",
    "\n",
    "results = []\n",
    "for question,embedding in zip(question_strs,embeddings):\n",
    "\n",
    "    query = {\n",
    "             \"BMX_Test\":question}\n",
    "    x = idx.search(query=query,\n",
    "                top_k=10)\n",
    "    res = x.keys[:10]\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_index(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order unaware metrics : {'avg precision@10': 0.8364, 'avg recall@10': 0.8388, 'avg F1@10': 0.8368000000000001}\n",
    "order aware metrics   : {'avg mrr': 0.33138762975560593, 'avg ndcg': 0.877741837170104, 'mean avg precision': 0.8128007936507937}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLADE then IndexFlat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_data(model_name):\n",
    "    with open(f'embeddings/{model_name}/hard/500/df.pkl', 'rb') as file: hotpot_qa_df = pickle.load(file)\n",
    "    with open(f'embeddings/{model_name}/hard/500/contexts.pkl', 'rb') as file: contexts = pickle.load(file)\n",
    "\n",
    "    questions = hotpot_qa_df[\"question\"].tolist()\n",
    "\n",
    "\n",
    "    return [q.embedding for q in questions],contexts\n",
    "\n",
    "dense_model = \"dunzhang/stella_en_400M_v5\"\n",
    "dense_questions,dense_contexts = load_data(dense_model)\n",
    "\n",
    "sparse_model = \"naver/splade-v3\"\n",
    "sparse_questions,sparse_contexts = load_data(sparse_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
